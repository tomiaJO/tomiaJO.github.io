<!DOCTYPE html>
<html lang="en-US">

<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="author" content="Tamas Koncz">
<meta name="description" content="Welcome to the first post in my ‘The Big Bang Theory (Tidy)Text analysis’ mini-series, and with that, to the very first post on my blog!
Recently I was taking a class on ‘Data Science on Unstructured Text Data’, for which our final project was to create a tidyverse text analysis of any topic of our choice.
While originally I set my sight on some deep topics, I ended up with something lighter this time.">

<meta property="og:title" content="TBBT (Tidy)Text analysis 1 - IMDB scores and character mentions" />
<meta property="og:description" content="Welcome to the first post in my ‘The Big Bang Theory (Tidy)Text analysis’ mini-series, and with that, to the very first post on my blog!
Recently I was taking a class on ‘Data Science on Unstructured Text Data’, for which our final project was to create a tidyverse text analysis of any topic of our choice.
While originally I set my sight on some deep topics, I ended up with something lighter this time." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/tbbt-imdb-score-regression/" />



<meta property="article:published_time" content="2018-04-01T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2018-04-01T00:00:00&#43;00:00"/>












<title>


     TBBT (Tidy)Text analysis 1 - IMDB scores and character mentions 

</title>
<link rel="canonical" href="/blog/tbbt-imdb-score-regression/">







<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/styles/default.min.css">




<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Ubuntu+Mono:400,400i,700,700i|Raleway:500">



    
    <link rel="stylesheet" href="/css/reset.css?t=2018-10-23%2015%3a16%3a39.1802848%20%2b0200%20CEST%20m%3d%2b0.202437801">
    <link rel="stylesheet" href="/css/pygments.css?t=2018-10-23%2015%3a16%3a39.1802848%20%2b0200%20CEST%20m%3d%2b0.202437801">
    <link rel="stylesheet" href="/css/main.css?t=2018-10-23%2015%3a16%3a39.1802848%20%2b0200%20CEST%20m%3d%2b0.202437801">
    
        <link rel="stylesheet" href="/css/override.css?t=2018-10-23%2015%3a16%3a39.1802848%20%2b0200%20CEST%20m%3d%2b0.202437801">
    




<link rel="shortcut icon"

    href="/img/leaf.ico"

>








</head>


<body lang="en">

<section class="header">
    <div class="container">
        <div class="content">
            
                
                
                
                
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                
                <a href="/"><img class="avatar" src="/img/kutyik.png" srcset="/img/kutyik.png 1x"></a>
            
            <a href="/"><div class="name">Tamas Koncz</div></a>
            
            <nav>
                <ul>
                    
                        <li class="nav-blog"><a href="/blog/"><span>Blog</span></a></li>
                    
                        <li class="nav-about"><a href="/about/"><span>About</span></a></li>
                    
                </ul>
            </nav>
        </div>
    </div>
</section>

<section class="icons">
    <div class="container">
        <div class="content">
        
            <a href="//github.com/tomiaJO" target="_blank" rel="noopener"><img class="icon" src="/img/github.svg" alt="github" /></a>
        

        

	

        

        

        
            <a href="//linkedin.com/in/tamaskoncz/" target="_blank" rel="noopener"><img class="icon" src="/img/linkedin.svg" alt="linkedin" /></a>
        

        

        

        

        

        
            <a href="mailto:t.koncz@gmail.com"><img class="icon" src="/img/email.svg" alt="email" /></a>
        

        

        
        </div>
    </div>
</section>


<section class="main post non-narrow zero-top-spacing">
    <div class="container">
        <div class="content">
            <div class="front-matter">
                <div class="title-container">
                    <div class="page-heading">

    TBBT (Tidy)Text analysis 1 - IMDB scores and character mentions

</div>

                    <div class="initials"><a href="/">ad</a></div>
                </div>
                <div class="meta">
                    
                    <div class="date" title='Sun Apr 1 2018 00:00:00 UTC'>Apr 1, 2018</div>
                    
                    
		    <div class="reading-time"><div class="middot"></div>7 minutes read</div>
                    
                </div>
            </div>
            <div class="markdown">
                <p>Welcome to the first post in my ‘The Big Bang Theory (Tidy)Text analysis’ mini-series, and with that, to the <em>very first post on my blog</em>!</p>
<p>Recently I was taking a class on ‘Data Science on Unstructured Text Data’, for which our final project was to create a tidyverse text analysis of any topic of our choice.</p>
<p>While originally I set my sight on some deep topics, I ended up with something lighter this time. What you need to know about me is that I’m quite addicted to spending my time in front of different TV series. TBBT used to be one of my favorites - and while the series lost some of its brilliance over the years, I still thought its body of many seasons should be great material for my analysis (also, lucky for me, Silicon Valley can fill in for my need of some good nerdy humor).</p>
<p>For this post what I was interested in is if the frequency of certain names appearing has some connection to an episode’s success (in this case, measured by the average rating of users on <a href="https://imdb.com">IMDB</a>).<br />
For the purposes of this excercise, names can appear in any format - the point is characters being mentioned in one way or another.</p>
<p>During this post, I’ll share bits of code to guide through my analysis - if you are interested in more, please refer to my <a href="https://github.com/tomiaJO/TEXT_MINING_FINAL_PROJECT">github page</a>.</p>
<p><strong>Let’s dive into the details!</strong></p>
<p>The data originally looked like as below<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. The full spoken text for each episode was one row, marked by ‘episode_id’. The data did not contain any metadata about speakers, or any contextual information, just the senteces that characters said during the show.</p>
<p>A glimpse into how it looked when I started:</p>
<table>
<thead>
<tr class="header">
<th align="left">episode_id</th>
<th align="left">episode_full_text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">s1e1</td>
<td align="left">So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slit…</td>
</tr>
<tr class="even">
<td align="left">s1e2</td>
<td align="left">Here we go. Pad thai, no peanuts. But does it have peanut oil? I’m not sure. Everyone keep an eye on Howard in case he starts…</td>
</tr>
<tr class="odd">
<td align="left">s1e3</td>
<td align="left">Alright, just a few more feet. And… here we are, gentlemen, the Gates of Elzebob. Good Lord. Don’t panic. This is what the …</td>
</tr>
</tbody>
</table>
<p>The first step I had to do is extract all words from the above format, using the unnest_tokens function:</p>
<pre class="r"><code>words_by_episode &lt;- full_episode_text %&gt;%
                      unnest_tokens(output = word, input = episode_full_text)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">episode_id</th>
<th align="left">word</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">s1e1</td>
<td align="left">so</td>
</tr>
<tr class="even">
<td align="left">s1e1</td>
<td align="left">if</td>
</tr>
<tr class="odd">
<td align="left">s1e1</td>
<td align="left">a</td>
</tr>
<tr class="even">
<td align="left">s1e1</td>
<td align="left">photon</td>
</tr>
<tr class="odd">
<td align="left">s1e1</td>
<td align="left">is</td>
</tr>
</tbody>
</table>
<p>The only “words” that I needed were the character names mentioned. However, simply selecting lines based on whether they contain names or not would not work.<br />
Let’s consider the below examples of “sheldon” and “raj”:</p>
<pre class="r"><code>words_by_episode %&gt;%
  filter(str_detect(word, &quot;raj&quot;)) %&gt;%
  select(word) %&gt;%
  unique() %&gt;%
  rename(&quot;Lines containing &#39;raj&#39;&quot; = &quot;word&quot;) %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Lines containing ‘raj’</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">raj</td>
</tr>
<tr class="even">
<td align="left">rajesh</td>
</tr>
<tr class="odd">
<td align="left">raj’s</td>
</tr>
<tr class="even">
<td align="left">trajectory</td>
</tr>
<tr class="odd">
<td align="left">rajasthan</td>
</tr>
<tr class="even">
<td align="left">maharaja</td>
</tr>
<tr class="odd">
<td align="left">rajesh’s</td>
</tr>
<tr class="even">
<td align="left">raj’ll</td>
</tr>
</tbody>
</table>
<pre class="r"><code>words_by_episode %&gt;%
  filter(str_detect(word, &quot;sheldon&quot;)) %&gt;%
  select(word) %&gt;%
  unique() %&gt;%
  rename(&quot;Lines containing &#39;sheldon&#39;&quot; = &quot;word&quot;) %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Lines containing ‘sheldon’</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">sheldon</td>
</tr>
<tr class="even">
<td align="left">sheldon’s</td>
</tr>
<tr class="odd">
<td align="left">sheldonmano</td>
</tr>
<tr class="even">
<td align="left">sheldons</td>
</tr>
<tr class="odd">
<td align="left">sheldonectomy</td>
</tr>
<tr class="even">
<td align="left">sheldonopolis</td>
</tr>
<tr class="odd">
<td align="left">sheldonian</td>
</tr>
<tr class="even">
<td align="left">sheldonoscopy</td>
</tr>
<tr class="odd">
<td align="left">sheldony</td>
</tr>
</tbody>
</table>
<p>We can see that names appear in many different format throught the text.<br />
To handle this, I create a vector containing regular expressions, custom-made for each name. (For most names this is still simply the name - I had to review one-by-one to decide on the right approach.)</p>
<pre class="r"><code>first_names_for_regex &lt;- c(&quot;howard&quot;,     &quot;penny&quot;,      &quot;leonard&quot;,     &quot;sheldon&quot;,
                           &quot;^raj[e&#39;]?&quot;,  &quot;^(sh)?amy&quot;,  &quot;^bern[ias]&quot;,  &quot;stuart&quot;,
                           &quot;zack&quot;,       &quot;emil[ey]&quot;,   &quot;leslie&quot;,      &quot;barry&quot;,
                           &quot;priya&quot;,      &quot;stephanie&quot;,  &quot;lucy&quot;)</code></pre>
<p>This vector can be merged into one regex, that we can use for filtering on the “word” column:</p>
<pre class="r"><code>first_names_regex &lt;- paste0(first_names_for_regex, collapse = &#39;|&#39;)
first_names_regex &lt;- paste(&quot;(&quot;, first_names_regex, &quot;)&quot;, sep = &quot;&quot;)

fnames_in_episodes &lt;- words_by_episode %&gt;%
                        filter(str_detect(word, first_names_regex))</code></pre>
<p>Given formatting was all over the place (see below…), as a next step I’ve applied some standardization.</p>
<pre><code>##  [1] &quot;leonard&quot;          &quot;sheldon&quot;          &quot;penny&quot;           
##  [4] &quot;sheldon&#39;s&quot;        &quot;howard&quot;           &quot;penny&#39;s&quot;         
##  [7] &quot;raj&quot;              &quot;rajesh&quot;           &quot;leslie&quot;          
## [10] &quot;leonard&#39;s&quot;        &quot;howard&#39;s&quot;         &quot;raj&#39;s&quot;           
## [13] &quot;sheldonmano&quot;      &quot;barry&quot;            &quot;sheldons&quot;        
## [16] &quot;stephanie&quot;        &quot;stephanie&#39;s&quot;      &quot;sheldonectomy&quot;   
## [19] &quot;bernie&quot;           &quot;rajasthan&quot;        &quot;stuart&quot;          
## [22] &quot;stuart&#39;s&quot;         &quot;leslie&#39;s&quot;         &quot;emile&quot;           
## [25] &quot;bernadette&quot;       &quot;bernadette.she&#39;s&quot; &quot;bernadette&#39;s&quot;    
## [28] &quot;sheldonopolis&quot;    &quot;zack&quot;             &quot;amy&quot;             
## [31] &quot;amy&#39;s&quot;            &quot;shamy&quot;            &quot;sheldonian&quot;      
## [34] &quot;priya&quot;            &quot;priya&#39;s&quot;          &quot;zack&#39;s&quot;          
## [37] &quot;bernie&#39;s&quot;         &quot;rajesh&#39;s&quot;         &quot;leonardville&quot;    
## [40] &quot;leonardstan&quot;      &quot;leonardwood&quot;      &quot;leonards&quot;        
## [43] &quot;koothrapenny&quot;     &quot;emily&quot;            &quot;raj&#39;ll&quot;          
## [46] &quot;amygdala&quot;         &quot;lucy&quot;             &quot;lucy&#39;s&quot;          
## [49] &quot;emily&#39;s&quot;          &quot;sheldonoscopy&quot;    &quot;koothrapemily&quot;   
## [52] &quot;amyâ&quot;             &quot;sheldony&quot;         &quot;pennysaver&quot;      
## [55] &quot;amy.â&quot;            &quot;bernstein&quot;        &quot;howardâ&quot;         
## [58] &quot;leonardo&quot;         &quot;emily.â&quot;          &quot;emilyâ&quot;          
## [61] &quot;bernardino&quot;       &quot;bernatrix&quot;</code></pre>
<pre class="r"><code>first_names &lt;- c(&quot;Howard&quot;,  &quot;Penny&quot;,    &quot;Leonard&quot;,     &quot;Sheldon&quot;,
                 &quot;Raj&quot;,     &quot;Amy&quot;,       &quot;Bernadette&quot;, &quot;Stuart&quot;,
                 &quot;Zack&quot;,    &quot;Emily&quot;,     &quot;Leslie&quot;,     &quot;Barry&quot;,
                 &quot;Priya&quot;,   &quot;Stephanie&quot;, &quot;Lucy&quot;)

first_names_mapping &lt;- data.table(regex = first_names_for_regex,
                                  name  = first_names)

len_first_names_mapping &lt;- first_names_mapping %&gt;% nrow()
fnames_in_episodes$name &lt;- &quot;&quot;

for(i in c(1:len_first_names_mapping)) {
  fnames_in_episodes &lt;- fnames_in_episodes %&gt;%
                          mutate(name = ifelse(str_detect(word, first_names_mapping[i, regex]), 
                                               first_names_mapping[i, name], 
                                               name))
}</code></pre>
<p>A small glimpse into the format we arrived at:</p>
<pre class="r"><code>fnames_in_episodes %&gt;%
  filter(name == &quot;Leonard&quot;) %&gt;%
  select(word, name) %&gt;%
  unique() %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">word</th>
<th align="left">name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">leonard</td>
<td align="left">Leonard</td>
</tr>
<tr class="even">
<td align="left">leonard’s</td>
<td align="left">Leonard</td>
</tr>
<tr class="odd">
<td align="left">leonardville</td>
<td align="left">Leonard</td>
</tr>
<tr class="even">
<td align="left">leonardstan</td>
<td align="left">Leonard</td>
</tr>
<tr class="odd">
<td align="left">leonardwood</td>
<td align="left">Leonard</td>
</tr>
<tr class="even">
<td align="left">leonards</td>
<td align="left">Leonard</td>
</tr>
<tr class="odd">
<td align="left">leonardo</td>
<td align="left">Leonard</td>
</tr>
</tbody>
</table>
<p>Let’s count how many times names appear per episode, and then spread the dataset to the wide format, which can be fed into a predictive model:</p>
<pre class="r"><code>episodes &lt;- fnames_in_episodes %&gt;%
              select(-word) %&gt;%
              group_by(episode_id, name) %&gt;%
              summarize(name_appear_count = n()) %&gt;%
              tidyr::spread(key = name, value = name_appear_count, fill = 0)</code></pre>
<p><img src="/blog/2018-04-01-tbbt-imdb-score-prediction_files/figure-html/unnamed-chunk-11-1.png" width="1440" /></p>
<p>We can then just join the IMDB scores based on episode ids:<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>episodes_w_scores &lt;- imdb_scores %&gt;%
                       inner_join(episodes, by = &quot;episode_id&quot;) %&gt;%
                       select(-Season, -Episode, -Title)</code></pre>
<p>Now that the data is in shape, it’s time to do what we are here for - figuring out who has the best impact on those IMDB scores!</p>
<p>My approach here is very simple: I’m going to run a glm regression, with on a training set of 75% of the original data. The remaining 25% will be used as a validation set, that helps ensure that the model did not overfit.<br />
The goal is not to build the best predictive model, as what I’m interested in is the coefficients. The simpler model, the more interpretable they will be.</p>
<pre class="r"><code>training_ratio &lt;- 0.75

set.seed(93)
train_indices &lt;- createDataPartition(y = episodes_w_scores[[&quot;IMDB Score&quot;]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)

data_train &lt;- episodes_w_scores[train_indices, ]
data_test  &lt;- episodes_w_scores[-train_indices, ]</code></pre>
<pre class="r"><code>train_control &lt;- trainControl(method = &quot;none&quot;)

set.seed(93)
glm_fit &lt;- train(`IMDB Score` ~ . -episode_id,
                 method = &quot;glm&quot;,
                 data = data_train,
                 trControl = train_control,
                 preProcess = c(&quot;center&quot;, &quot;scale&quot;))</code></pre>
<p>When I ran the model I deemed it good enough to draw some conclusions, so it’s time to take a look at those coefficients (see end notes on model robustness).</p>
<pre class="r"><code>coefficients &lt;- coef(glm_fit$finalModel)[-1]
coefficients &lt;- data.frame(Name      = names(coefficients), 
                           beta      = coefficients,
                           row.names = NULL) %&gt;%
                mutate(Name = reorder(Name, beta)) %&gt;%
                mutate(Impact = ifelse(beta &gt;= 0, &quot;+&quot;, &quot;-&quot;))</code></pre>
<p>Putting the results into a nice visual:<br />
<img src="/blog/2018-04-01-tbbt-imdb-score-prediction_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Let’s interpret the above: the most positive impact comes from Sheldon, Howard, and Penny, while the most negative is from Emily, Stuart, Leonard, and Amy.<br />
Every time Sheldon is mentioned in some context, we expect an 0.08 higher IMDB score for the given episode. Every time Amy is mentioned in some context, we expect an 0.06 lower IMDB score for the given episode. <img src="/blog/2018-04-01-tbbt-imdb-score-prediction_files/figure-html/unnamed-chunk-18-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>This should give us some insight into who are the viewer favorites of the show - but we should not take the results (just as the topic) too seriously.<br />
The methodology is clearly not very robust - this is, after all, is more for fun than for “science”.</p>
<hr />
<p>Notes:</p>
<p>In the post I have not gone into detail about robustness of the model’s fit, so let’s spend a minute on that here.</p>
<p>One thing to look at is the RMSE on the set-aside test set vs the standard deviation of IMDB Scores:</p>
<pre><code>## [1] &quot;RMSE: 0.4, Std. Dev.: 0.43&quot;</code></pre>
<p>That the values are in the same ballpark signals that even if just weakly, but the model does have some descriptive power.</p>
<p><img src="/blog/2018-04-01-tbbt-imdb-score-prediction_files/figure-html/unnamed-chunk-20-1.png" width="384" style="display: block; margin: auto auto auto 0;" /></p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I’m working on a post that describes how to gather subtitle data and turn it into a tidy format.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Data was downloaded from <a href="https://www.opensubtitles.org/en/ssearch/sublanguageid-eng/idmovie-27926" class="uri">https://www.opensubtitles.org/en/ssearch/sublanguageid-eng/idmovie-27926</a>, where they nicely collected live IMDB scores for all episodes of TBBT.<a href="#fnref2">↩</a></p></li>
</ol>
</div>

                <br>
		<p class="back-to-posts"><a href="/blog/">Back to posts</a></p>
            </div>
            <br>
            <div class="disqus">
                
            </div>
            
        </div>
    </div>
</section>



<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-123-45', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>



  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.11.0/highlight.min.js"></script>
  

  <script type="text/javascript">
    hljs.initHighlightingOnLoad();
  </script>





</body>
</html>

